#!/usr/bin/env python

from __future__ import print_function

import argparse
import ast
import contextlib
import math
import os
import re
import subprocess
import sys

from CRABAPI.RawCommand import crabCommand as crab

class void():
    def write(self, x):
        pass


@contextlib.contextmanager
def silence():
    original = sys.stdout
    sys.stdout = void()
    yield
    sys.stdout = original


sites = {
    'T2_US_MIT': '/store/user/',
    'T2_CH_CERN': '/store/group/phys_heavyions/',
    }

# to be tuned (GEN_SIM, DIGI2RAW, etc..)
steps = ['GENSIM', 'DIGIRAW', 'RECO', 'FOREST', 'CUSTOM']
guess = ['GEN_SIM', 'DIGI2RAW', 'RECO', 'Forest', 'CUSTOM']
target = {
    'GENSIM': 200,
    'DIGIRAW': 200,
    'RECO': 200,
    'FOREST': 1000,
    'CUSTOM': 400,
    }

def generate_config(params):
    config = {
        'General': {
            'transferOutputs': 'transfer',
            'requestName': 'request',
            'workArea': 'workspace' },
        'JobType': {
            'pluginName': 'type',
            'psetName': 'pset',
            'inputFiles': 'input',
            'maxMemoryMB': 'memory',
            'maxJobRuntimeMin': 'runtime',
            'priority': 'priority' },
        'Data': {
            'ignoreLocality': 'ignorelocal',
            'inputDataset': 'dataset',
            'inputDBS': 'dbs',
            'splitting': 'split',
            'unitsPerJob': 'units',
            'totalUnits': 'total',
            'lumiMask': 'lumimask',
            'outLFNDirBase': 'lfn',
            'outputPrimaryDataset': 'primarydataset',
            'publication': 'publish' },
        'User': {},
        'Site': {
            'blacklist': 'blacklist',
            'whitelist': 'whitelist',
            'storageSite': 'site' },
        'Debug': {
            'extraJDL': 'extrajdl' },
        }

    output = 'config_crab_{}.py'.format(params['request'].lower())

    with open(output, 'w') as f:
        f.write('from WMCore.Configuration import Configuration\n')
        f.write('config = Configuration()\n')
        f.write('\n')

        for section, options in config.iteritems():
            f.write('config.section_(\'{}\')\n'.format(section))
            for option, key in options.iteritems():
                if key not in params or params[key] is None:
                    continue

                if isinstance(params[key], str):
                    f.write('config.{}.{} = \'{}\'\n'.format(
                        section, option, params[key]))
                else:
                    f.write('config.{}.{} = {}\n'.format(
                        section, option, str(params[key])))
            f.write('\n')

    print('generated config:', output)


def collect_production_parameters(params):
    # split jobs based on filter efficiency and total events
    # target 200 events per job (GENSIM)
    params['split'] = 'EventBased'
    params['units'] = int(math.ceil(200 / params['filter']))
    params['total'] = int(params['units'] * math.ceil(params['total'] / 200))

    # devise dataset name if not given
    if params['dataset'] is None:
        params['dataset'] = (os.path.splitext(params['pset'])[0]
            .partition('_cff_py')[0].partition('_cfi_py')[0])
    if params['dataset'].count('GEN'):
        params['dataset'] = re.sub('_GEN_?SIM(_PU)?', '', params['dataset'], 1)
    params['primarydataset'] = params['dataset']
    params['dataset'] = None

    # generate request name
    params['request'] = '{}_GENSIM_{}_{}_v{}'.format(
        params['primarydataset'],
        os.environ['CMSSW_VERSION'][5:].replace('_', ''),
        params['tag'],
        params['version']).replace('__', '_')

    params['type'] = 'PrivateMC'


def collect_analysis_parameters(params):
    if params['dataset'] is None:
        # find output dataset
        with silence():
            crab_resp = crab('status', dir=params['dir'])
        params['dataset'] = eval(crab_resp['outdatasets'])[0]

    if params['dbs'] is None:
        params['dbs'] = 'phys03'

    # check das for dataset presence
    if not params['ignoredas'] and params['whitelist'] is not None:
        try:
            das_resp = ast.literal_eval(subprocess.check_output([
                'dasgoclient', '-json',
                '-query={} instance=prod/{} site'.format(
                    params['dataset'], params['dbs'])]))
        except subprocess.CalledProcessError:
            print('warning: das query failed!')
            raise
        else:
            sites_info = [s['site'][0] for s in das_resp if
                any(r in s['das']['services'][0] for r in ['combined', 'dbs3'])]

            try:
                sites_available = [s for s in sites_info if
                    s['kind'] in ['Disk', 'original placement']]
            except KeyError:
                # query for block
                print('warning: storage type not available [block query?]')
                sites_available = [s['site'][0] for s in das_resp if
                    'phedex' in s['das']['services'][0]]

            if not any(s['name'] in params['whitelist'] for
                    s in sites_available):
                print('warning: dataset not available in whitelisted sites:',
                      params['whitelist'])
                sites_list = [s['name'] for s in sites_available
                              if not any(s['name'].startswith(p)
                                  for p in ['T0_', 'T1_'])]
                print('   note: dataset availability:', sites_list)
                print('warning: changing whitelist accordingly')
                params['whitelist'] = sites_list

    if params['split'] is None:
        params['split'] = 'FileBased'

        if params['units'] is None:
            # rough splitting for jobs, based on events/job target
            das_resp = ast.literal_eval(subprocess.check_output([
                'dasgoclient', '-query={} instance=prod/{} summary'.format(
                    params['dataset'], params['dbs'])]))[0]
            params['units'] = max(1, int(math.floor(target[params['step']]
                / (das_resp['nevents'] / das_resp['nfiles']))))

    # generate request name
    params['request'] = '{}_{}_{}_{}_v{}'.format(
        (params['dataset'].split('/')[1] if 'Run201' not in params['dataset']
            else '_'.join(params['dataset'].split('/')[1:3])),
        params['step'],
        os.environ['CMSSW_VERSION'][5:].replace('_', ''),
        params['tag'],
        params['version']).replace('__', '_')

    params['type'] = 'Analysis'


def check_parameters(params):
    if params['step'] is None or params['step'] not in steps:
        print('invalid step')
        return False
    if params['lfn'] is None and params['site'] not in sites:
        print('lfn not known for site:', params['site'])
        return False

    if params['step'] == 'GENSIM':
        if params['total'] <= 0:
            print('invalid total events (-t N):', params['total'])
            return False
        if params['filter'] <= 0:
            print('invalid filter efficiency:', params['filter'])
            return False
        if params['split'] not in ['EventBased', None]:
            print('splitting for GENSIM must be EventBased')
            return False
        if params['dbs'] is not None:
            print('warning: GENSIM step has dbs set - will be ignored')
            params['dbs'] = None
        if params['nonlocal']:
            print('warning: ignoring nonlocal setting for GENSIM')
            params['nonlocal'] = False
    else:
        if params['dataset'] is None:
            if params['dir'] is None:
                print('no input dataset')
                return False
            elif params['dbs'] is not None:
                print('warning: ignoring dbs instance for private dataset')

        if params['split'] is 'Automatic':
            if params['units'] < 180:
                print('invalid target runtime for automatic splitting (-u N)')
                return False
        elif params['split'] is not None:
            if params['units'] is None:
                print('manual splitting, but no units/job')
                return False
        elif params['ignoredas']:
            if params['units'] is None:
                print('automatic splitting, but skipping DAS query')
                return False

    return True


def collect_parameters(params):
    if params['step'] is None:
        if any(g in params['pset'] for g in guess):
            step = next(g for g in guess if g in params['pset'])
            params['step'] = steps[guess.index(step)]
        elif params['dir'] is not None or params['dataset'] is not None:
            params['step'] = 'CUSTOM'

        if params['step'] is not None:
            print('guessing this is ..', params['step'])

    if params['global'] == True:
        params['dbs'] = 'global'

    if not check_parameters(params):
        sys.exit()

    if params['lfn'] is None:
        params['lfn'] = sites[params['site']]
    params['lfn'] += os.environ['USER']

    if params['nonlocal']:
        params['ignorelocal'] = True
        params['ignoredas'] = True
        params['whitelist'] = ['T2_US_*']
    elif params['whitelist'] is not None:
        params['whitelist'] = params['whitelist'].split(',')
        params['extrajdl'] = ['+CMS_ALLOW_OVERFLOW=False']

    if params['step'] == 'GENSIM':
        collect_production_parameters(params)
    else:
        collect_analysis_parameters(params)

    generate_config(params)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('pset')

    parser.add_argument('-d', '--dir')
    parser.add_argument('-s', '--step')

    parser.add_argument('-f', '--filter', default=1., type=float)
    parser.add_argument('-m', '--memory', default=4000, type=int)
    parser.add_argument('-t', '--total', default=-1, type=int)
    parser.add_argument('-v', '--version', default=1, type=int)

    parser.add_argument('-g', '--global', action='store_true')
    parser.add_argument('-i', '--input', action='append')
    parser.add_argument('-p', '--priority', type=int)
    parser.add_argument('-u', '--units', type=int)

    parser.add_argument('-w', '--whitelist', nargs='?', const='T2_US_MIT')

    parser.add_argument('--ignoredas', action='store_true')
    parser.add_argument('--nonlocal', action='store_true')
    parser.add_argument('--publish', action='store_false')
    parser.add_argument('--site', default='T2_US_MIT')
    parser.add_argument('--tag', default='')
    parser.add_argument('--transfer', action='store_false')
    parser.add_argument('--workspace', default='{}/crab'.format(
        os.environ['HOME']))

    parser.add_argument('--split', choices={
        'Automatic', 'EventBased', 'FileBased', 'LumiBased',
        'EventAwareLumiBased'})

    parser.add_argument('--dataset')
    parser.add_argument('--dbs')
    parser.add_argument('--lfn')
    parser.add_argument('--lumimask')
    parser.add_argument('--runtime', type=int)

    args = parser.parse_args()

    collect_parameters(vars(args))
